{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gbgp0NXdc137"
   },
   "source": [
    "# Project 1: Web scraping and basic summarization\n",
    "*University of Ljubljana, Faculty for computer and information science* <br />\n",
    "*Course: Introduction to data science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkYxh9TVc13-"
   },
   "source": [
    "In this Project you need to implement missing parts of this Jupyter notebook. All the code in the notebook must be reproducible and runnable, so include instructions for the environment setup or other specifics needed to run the notebook. The overview of the repository and setup should be put to README.md\n",
    "\n",
    "The idea of this Project is to automatically retrieve structured data from pages [rtvslo.si](https://www.rtvslo.si) or [24ur.com](https://www.24ur.com). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmDFHSpSc14M"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6cCtBRGc14R"
   },
   "source": [
    "Write instructions how to setup the environment to run this notebook, which libraries are installed, etc. Also provide installation sources.\n",
    "\n",
    "`TODO: environment setup description`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3ooqpJOc14V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Load all the libraries needed for running the code chunks below\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "from helper_f import links,  get_article_data,  get_article_comments,  edit_and_join, get_data_skit, get_data_dostopno, get_data_enostavno, make_json_format, change_day_published\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#COMMENTED TO CAN NOT ACCIDENTALY OVERWRITE URLS WITH  NEW, MORE RECENT ONES.\n",
    "\n",
    "main_URL = \"https://www.rtvslo.si\"\n",
    "num = 1000\n",
    "main_key = \"Koronavirus\"\n",
    "\n",
    "\n",
    "links(main_URL, main_key, num) #get 1000 article urls for search key \"Koronavirus\" and write it into article_urls.txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIXBDoF1c14Z"
   },
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KbM1Lx0c14a"
   },
   "source": [
    "\n",
    "\n",
    "JSON Schema:\n",
    "\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"author\": [\"author_1\", \"author_2\",...],\n",
    "    \"day_published\": \"DD.MM.YYYY\",\n",
    "    \"changed_later\": \"YES\"/\"NO\", \n",
    "    \"title\": \"article_title\",\n",
    "    \"subtitle\": \"article_subtitle\",\n",
    "    \"tags\": [\"tag_1\", \"tag_2\",...],\n",
    "    \"section_tag\": \"section_tag\"\n",
    "    \"content\": \"article_text\",\n",
    "    \"comments\": [\n",
    "        {\n",
    "            \"user\": \"user_name\",\n",
    "            \"date_hour\": [\"DD.MM.YYYY\"; \"MM:HH\"],\n",
    "            \"grade\": comment_grade(as number),\n",
    "            \"reply\": \"YES\"/\"NO\",\n",
    "            \"comment\": \"comment_text\",\n",
    "\n",
    "        },...\n",
    "    ],\n",
    "    \"hour_published\": \"MM:HH\"\n",
    "    \n",
    "  }, \n",
    "  {\n",
    "    ...\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuMR9RRWc14b"
   },
   "source": [
    "`TODO: definition and short description of JSON data schema of the extracted data.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed structure of the implementation is below. You can also import code from the accompanying *.py* files in the repository, so the code is clearer or organize code completely by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scraping\n",
      "END OF MAIN SCRAPING!!!\n",
      "Starting to get data from article:  SKIT\n",
      "Starting to get data from article:  SKIT\n",
      "END OF SKIT SCRAPING!!!\n",
      "END OF DOSTOPNO SCRAPING!!!\n",
      "END OF ENOSTAVNO SCRAPING!!!\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data_urls = [] #normal rtvslo articles with no SKIT, ENOSTAVNO, DOSTOPNO articles\n",
    "\n",
    "enostavno = []\n",
    "skit = []\n",
    "dostopno = []\n",
    "i = 0\n",
    "print(\"Started scraping\")\n",
    "file = open('article_urls.txt', 'r', encoding='utf-8')\n",
    "lines = file.readlines()\n",
    "for line in lines: #urls \n",
    "    if \"/enostavno/\" in line:\n",
    "        enostavno.append(line[:-1])\n",
    "    elif \"/skit/\" in line:\n",
    "        skit.append(line[:-1]) #\\n has to be removed\n",
    "    elif \"/dostopno/\" in line:\n",
    "        dostopno.append(line[:-1])\n",
    "    else: #normal ones\n",
    "        data_urls.append(line[:-1])\n",
    "        #print(\"Starting to write url number \", )\n",
    "\n",
    "        \n",
    "u = len(data_urls)\n",
    "s = len(skit)\n",
    "d = len(dostopno)\n",
    "e = len(enostavno)\n",
    "        \n",
    "for j in range(u): #len data_urls\n",
    "    if os.path.isfile('json/{}.json'.format(j)): #check if article data was already scraped\n",
    "        pass\n",
    "    else: #get article and comments data\n",
    "\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text) = get_article_data(data_urls[j])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(j), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF MAIN SCRAPING!!!\")\n",
    "\n",
    "#scrapaj še za SKIT, DOSTOPNO, ENOSTAVNO in zapiši v json in potem naredi json edited. \n",
    "\n",
    "for i in range(s): #len skit\n",
    "    if os.path.isfile('json/{}.json'.format(u + i)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_skit(skit[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF SKIT SCRAPING!!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(d):\n",
    "    if os.path.isfile('json/{}.json'.format(u + s + i)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_dostopno(dostopno[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u + s), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF DOSTOPNO SCRAPING!!!\")\n",
    "\n",
    "\n",
    "for i in range(e):\n",
    "    if os.path.isfile('json/{}.json'.format(u + s + i + d)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_enostavno(enostavno[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u + s + d), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF ENOSTAVNO SCRAPING!!!\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json files fixed and joined into data.json file.\n"
     ]
    }
   ],
   "source": [
    "edit_and_join() #fix typos and join all single article json files into one big json file with all data\n",
    "print(\"Json files fixed and joined into data.json file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23 marec 2021'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla = \"23. marec 2021\".split(\" \")\n",
    "blabla = \"23. marec 2021\".replace('.','')\n",
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23 marec 2021'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla[1]= \"5\"\n",
    "blabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'change_date_published' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21640/3662270560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchange_date_published\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'change_date_published' is not defined"
     ]
    }
   ],
   "source": [
    "change_date_published(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGR3NslWc14c"
   },
   "outputs": [],
   "source": [
    "# Implement news article page parsing\n",
    "\n",
    "def parse_news_article(url):\n",
    "    ...\n",
    "    return news_article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uIHflU3nc14e"
   },
   "outputs": [],
   "source": [
    "# Implement search results page parsing \n",
    "\n",
    "def parse_search_results(url):\n",
    "    ...\n",
    "    return results, next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycXUr0nSc14h"
   },
   "outputs": [],
   "source": [
    "# Implement parsing, data merging and final representation\n",
    "\n",
    "def search():\n",
    "    ...\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp0FEp9hc14m"
   },
   "outputs": [],
   "source": [
    "# Main program and data export into a JSON\n",
    "\n",
    "MAIN_URL = \"https://www.rtvslo.si\"\n",
    "\n",
    "data = search()\n",
    "# Save data to UTF-8 encoded JSON\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4y30zo_c14p"
   },
   "source": [
    "## Basic summarization\n",
    "\n",
    "Prepare and show at least five basic visualizations of the extracted data as presented in the chapter *Summarizing data - the basics* of the course's e-book. Explain each visualization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPjO-LSvc14q"
   },
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1jQe5ELc14s"
   },
   "source": [
    "### Visualization 1\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpGWh7BPc14t"
   },
   "outputs": [],
   "source": [
    "# Visualization 1 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SVSulJQc14v"
   },
   "source": [
    "### Visualization 2\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GvtoWSDc14v"
   },
   "outputs": [],
   "source": [
    "# Visualization 2 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "po4uso66c14x"
   },
   "source": [
    "### Visualization 3\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xKMH6rjc14y"
   },
   "outputs": [],
   "source": [
    "# Visualization 3 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ibr8rBN_c14z"
   },
   "source": [
    "### Visualization 4\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIa6vaMOc140"
   },
   "outputs": [],
   "source": [
    "# Visualization 4 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riFWfhi8c142"
   },
   "source": [
    "### Visualization 5\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vv3yRILc15B"
   },
   "outputs": [],
   "source": [
    "# Visualization 5 code\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Web_Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "upv-project_1",
   "language": "python",
   "name": "upv-project_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
