{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gbgp0NXdc137"
   },
   "source": [
    "# Project 1: Web scraping and basic summarization\n",
    "*University of Ljubljana, Faculty for computer and information science* <br />\n",
    "*Course: Introduction to data science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkYxh9TVc13-"
   },
   "source": [
    "\n",
    "\n",
    "The idea of this Project is to automatically retrieve structured data from pages [rtvslo.si](https://www.rtvslo.si)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmDFHSpSc14M"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6cCtBRGc14R"
   },
   "source": [
    "To setup environment we first need to install conda. Conda can be downloaded from: https://docs.conda.io/projects/conda/en/latest/user-guide/install/download.html . Then we need to make new evironment. I used name \"upv-project_1\" but it can be arbitrary name. Use commands from below to setup environment. \n",
    "\n",
    "`ENVIRONMENT SETUP DESCRIPTIONS:\n",
    "conda create --name upv-project_1\n",
    "conda activate upv-project_1\n",
    "conda install python\n",
    "conda install selenium\n",
    "conda install jupyter notebook\n",
    "ipython kernel install --name \"upv-project_1\" --user\n",
    "conda install pandas\n",
    "conda install requests\n",
    "conda install numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIXBDoF1c14Z"
   },
   "source": [
    "## Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape all whole pages until 1000th article with search key \"koronavirus\" from  [rtvslo.si](https://www.rtvslo.si) and save data in JSON format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KbM1Lx0c14a"
   },
   "source": [
    "JSON Schema:\n",
    "\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"author\": [\"author_1\", \"author_2\",...],\n",
    "    \"day_published\": \"DD.MM.YYYY\",\n",
    "    \"changed_later\": \"YES\"/\"NO\", \n",
    "    \"title\": \"article_title\",\n",
    "    \"subtitle\": \"article_subtitle\",\n",
    "    \"tags\": [\"tag_1\", \"tag_2\",...],\n",
    "    \"section_tag\": \"section_tag\"\n",
    "    \"content\": \"article_text\",\n",
    "    \"comments\": [\n",
    "        {\n",
    "            \"user\": \"user_name\",\n",
    "            \"date_hour\": [\"DD.MM.YYYY\"; \"MM:HH\"],\n",
    "            \"grade\": comment_grade(as number),\n",
    "            \"reply\": \"YES\"/\"NO\",\n",
    "            \"comment\": \"comment_text\",\n",
    "\n",
    "        },...\n",
    "    ],\n",
    "    \"hour_published\": \"MM:HH\"\n",
    "    \n",
    "  }, \n",
    "  {\n",
    "    ...\n",
    "]\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In block below are listed all libraries and helper functions needed. Helper functions are imported from helper_f.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Load all the libraries needed for running the code chunks below\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import numpy as np\n",
    "from helper_f import links,  get_article_data,  get_article_comments,  edit_and_join, get_data_skit, get_data_dostopno, get_data_enostavno, make_json_format, change_day_published\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get urls of all articles. Function \"links\" simulates typing search key \"koronavirus\" in search bar and navigates trough pages and collects urls of most recent articles and saves them into file \"article_urls.txt\". Every time we run it we can get new different urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMENTED TO CAN NOT ACCIDENTALY OVERWRITE URLS WITH  NEW, MORE RECENT ONES.\n",
    "\n",
    "main_URL = \"https://www.rtvslo.si\"\n",
    "num = 1000 #number of articles, max on rtvslo is 1000\n",
    "main_key = \"Koronavirus\"\n",
    "\n",
    "\n",
    "#links(main_URL, main_key, num) #get 1000 article urls for search key \"Koronavirus\" and write it into article_urls.txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read url by url from \"article_urls.txt\" file. I divided urls in 4 categories, based on webpage style and features. Majority of web pages is in let's say normal rtvslo article format (those also can have comments). But some of them are in others \"SKIT\", \"ENOSTAVNO\" and \"DOSTOPNO\" cathegory. Each of them have unique source code. Pages will be scraped depending on cathegory by functions: get_article_data (for normal articles), get_data_skit, get_data_dostopno and get_data_enostavno. Comments from \"normal\" rtvslo articles wil be collected with function get_article_comments. Data from each article will be saved into own .json file in json folder with make_json_format function . Before scraping each article script will check wether article was already scraped or not. So in case of some interuption it wont need to start from beginning again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started scraping\n",
      "END OF MAIN SCRAPING!!!\n",
      "END OF SKIT SCRAPING!!!\n",
      "END OF DOSTOPNO SCRAPING!!!\n",
      "END OF ENOSTAVNO SCRAPING!!!\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "data_urls = [] #normal rtvslo articles with no SKIT, ENOSTAVNO, DOSTOPNO articles\n",
    "\n",
    "enostavno = []\n",
    "skit = []\n",
    "dostopno = []\n",
    "i = 0\n",
    "print(\"Started scraping\")\n",
    "file = open('article_urls.txt', 'r', encoding='utf-8')\n",
    "lines = file.readlines()\n",
    "for line in lines: #urls \n",
    "    if \"/enostavno/\" in line:\n",
    "        enostavno.append(line[:-1])\n",
    "    elif \"/skit/\" in line:\n",
    "        skit.append(line[:-1]) #\\n has to be removed\n",
    "    elif \"/dostopno/\" in line:\n",
    "        dostopno.append(line[:-1])\n",
    "    else: #normal ones\n",
    "        data_urls.append(line[:-1])\n",
    "        #print(\"Starting to write url number \", )\n",
    "\n",
    "        \n",
    "u = len(data_urls)\n",
    "s = len(skit)\n",
    "d = len(dostopno)\n",
    "e = len(enostavno)\n",
    "        \n",
    "for j in range(u): #len data_urls\n",
    "    if os.path.isfile('json/{}.json'.format(j)): #check if article data was already scraped\n",
    "        pass\n",
    "    else: #get article and comments data\n",
    "\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text) = get_article_data(data_urls[j])\n",
    "        comments = get_article_comments(data_urls[j])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(j), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF MAIN SCRAPING!!!\")\n",
    "\n",
    "\n",
    "for i in range(s): #len skit\n",
    "    if os.path.isfile('json/{}.json'.format(u + i)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_skit(skit[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF SKIT SCRAPING!!!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(d):\n",
    "    if os.path.isfile('json/{}.json'.format(u + s + i)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_dostopno(dostopno[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u + s), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF DOSTOPNO SCRAPING!!!\")\n",
    "\n",
    "\n",
    "for i in range(e):\n",
    "    if os.path.isfile('json/{}.json'.format(u + s + i + d)): #check if article data was already scraped\n",
    "        pass\n",
    "    else:\n",
    "        (authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments) = get_data_enostavno(enostavno[i])\n",
    "        line_data = make_json_format(authors, title, subtitle, date, hour, change, article_tags, section_tag, text, comments)\n",
    "        \n",
    "        data.append(line_data)\n",
    "\n",
    "\n",
    "        #write to seperate .json file\n",
    "        with open(\"json/{}.json\".format(i + u + s + d), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(line_data, outfile, ensure_ascii=False)\n",
    "            \n",
    "print(\"END OF ENOSTAVNO SCRAPING!!!\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a lot of time to scrape all the data. I found some typing mistakes but decided not to scrape again but to fix already scraped data. Function in block below edit each .json file and then saves it into folder edited_json_files. It also merges all edited files togather into one big file called data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json files fixed and joined into data.json file.\n"
     ]
    }
   ],
   "source": [
    "edit_and_join() #fix typos and join all single article json files into one big json file with all the data.\n",
    "print(\"Json files fixed and joined into data.json file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4y30zo_c14p"
   },
   "source": [
    "## Basic summarization\n",
    "\n",
    "Prepare and show at least five basic visualizations of the extracted data as presented in the chapter *Summarizing data - the basics* of the course's e-book. Explain each visualization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPjO-LSvc14q"
   },
   "outputs": [],
   "source": [
    "# Read data from JSON\n",
    "\n",
    "data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1jQe5ELc14s"
   },
   "source": [
    "### Visualization 1\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpGWh7BPc14t"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization 1 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SVSulJQc14v"
   },
   "source": [
    "### Visualization 2\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GvtoWSDc14v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization 2 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "po4uso66c14x"
   },
   "source": [
    "### Visualization 3\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xKMH6rjc14y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization 3 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ibr8rBN_c14z"
   },
   "source": [
    "### Visualization 4\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIa6vaMOc140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization 4 code\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riFWfhi8c142"
   },
   "source": [
    "### Visualization 5\n",
    "\n",
    "`TODO: name the visualization and describe it`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Vv3yRILc15B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualization 5 code\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Web_Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "upv-project_1",
   "language": "python",
   "name": "upv-project_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
